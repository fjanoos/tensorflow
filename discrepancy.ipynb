{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# start a tensorflow session\n",
    "from pylab import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "DTYPE = tf.float32\n",
    "EPS = 1e-9\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "g = tf.Graph().as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0.00123191 \n",
      " [ 0.01531384  0.01517148  0.01503047  0.01489076  0.01475236  0.01461524\n",
      "  0.0144794   0.01434482  0.01421149  0.0140794   0.01394853  0.01381888\n",
      "  0.01369044  0.0135632   0.01343713  0.01331223  0.0131885   0.01306591\n",
      "  0.01294446  0.01282416  0.01270496  0.01258687  0.01246988  0.01235397\n",
      "  0.01223915  0.01212538  0.01201269  0.01190102  0.01179041  0.01168082\n",
      "  0.01157225  0.01146469  0.01135813  0.01125255  0.01114797  0.01104435\n",
      "  0.01094169  0.01083999  0.01073924  0.01063943  0.01054053  0.01044256\n",
      "  0.0103455   0.01024934  0.01015408  0.01005968  0.00996619  0.00987356\n",
      "  0.00978179  0.00969087  0.00960079  0.00951155  0.00942315  0.00933557\n",
      "  0.00924878  0.00916282  0.00907765  0.00899329  0.0089097   0.00882688\n",
      "  0.00874484  0.00866356  0.00858303  0.00850325  0.00842422  0.00834592\n",
      "  0.00826835  0.0081915   0.00811535  0.00803993  0.00796521  0.00789116\n",
      "  0.00781782  0.00774514  0.00767316  0.00760184  0.00753118  0.00746118\n",
      "  0.00739184  0.00732313  0.00725506  0.00718763  0.00712082  0.00705464\n",
      "  0.00698906  0.0069241   0.00685974  0.00679599  0.00673282  0.00667024\n",
      "  0.00660824  0.00654682  0.00648597  0.00642568  0.00636596  0.00630679\n",
      "  0.00624817  0.0061901   0.00613256  0.00607556]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TensorArray' object has no attribute 'stack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bbfc878abca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m221\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0metas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorArray' object has no attribute 'stack'"
     ]
    }
   ],
   "source": [
    "# the EG algorithsm\n",
    "def eg( loss, w, shift_rate=tf.constant(0, dtype=DTYPE), eta_0=None, max_its=100, tol=tf.constant(1e-6), step_rule='sqrt' ):\n",
    "    '''\n",
    "    do batch eg on the given loss tensor. w are the simplex weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loss: A function with returns losses\n",
    "    w: the parameter tensor\n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "    k: iteration counts    \n",
    "    weights: tf.TensorArray of weights\n",
    "    losses: TensorArray of losss\n",
    "    relchg: TensorArray of relative changes    \n",
    "    etas: tf.TensorArray of learning raters\n",
    "    max grads: tf.TensorArray of max gradients\n",
    "    '''    \n",
    "    \n",
    "    def _eg_step( k, weights, changes, losses, etas, max_grads ):\n",
    "        '''\n",
    "        eg step with no sleeping expert correction\n",
    "        # for sleeping expert do this \n",
    "        # awake_sum = tf.reduce_sum( tf.where( nan_mask, tf.zeros_like(w), w) )\n",
    "        # w_s = tf.where( nan_mask, tf.zeros_like(w), tf.divide( w, awake_sum ) )\n",
    "        \n",
    "        # theoretical optimal eta = sqrt( 2 log N / K ) / G_inf\n",
    "        '''\n",
    "        w = weights.read( k )\n",
    "        grad = tf.gradients( loss(w), w )[0]\n",
    "        nan_mask = tf.is_nan( grad )        \n",
    "        grad = tf.where( nan_mask, tf.zeros_like(grad, dtype=DTYPE), grad )\n",
    "        # find the new max gradient\n",
    "        G_inf = tf.maximum( max_grads.read(k), tf.reduce_max( tf.abs( grad ) ) )        \n",
    "        # pre-condition\n",
    "        grad = grad - tf.reduce_min( grad )             \n",
    "        if step_rule == 'constant':\n",
    "            eta = eta_0 / G_inf\n",
    "        elif step_rule == 'sqrt':\n",
    "            eta = eta_0 / G_inf / tf.sqrt( 1 +  tf.cast( k, dtype=DTYPE) )\n",
    "        elif step_rule == 'inv':\n",
    "            eta = eta_0 / G_inf / ( 1 + tf.cast( k, dtype=DTYPE) )\n",
    "        w_n = w * tf.exp( -eta  * grad )\n",
    "        w_n = w_n / tf.reduce_sum( w_n ) * (1 - shift_rate) + shift_rate     \n",
    "        # apply the updates\n",
    "        k = k + 1\n",
    "        etas = etas.write( k, eta )\n",
    "        weights = weights.write( k, w_n )\n",
    "        max_grads = max_grads.write( k, G_inf )\n",
    "        losses = losses.write( k, loss( weights.read(k ) ) )\n",
    "        changes = changes.write( k, tf.reduce_sum( tf.abs( weights.read(k) - weights.read(k-1) / tf.reduce_sum( tf.abs( weights.read(k-1) ) ) ) ) )\n",
    "        print_op = tf.Print( k, [k, eta, losses.read(k), changes.read(k)], 'k, eta, loss, change = \\t' )\n",
    "        with tf.control_dependencies( [print_op] ):            \n",
    "            return k, weights, changes, losses, etas, max_grads\n",
    "        \n",
    "    def _continue_cond( k, weights, changes, *args ):\n",
    "        chg_w = changes.read(k)\n",
    "        return tf.logical_and( k < max_its - 1,  chg_w > tol  ) \n",
    "    \n",
    "    with tf.name_scope('EG'):\n",
    "        if eta_0 is None:\n",
    "            eta_0 = tf.constant( np.sqrt( 2 * np.log( w.get_shape().as_list()[0] ) / max_its), dtype=DTYPE  )         \n",
    "        k = tf.constant( 0, dtype=tf.int32 )\n",
    "        weights = tf.TensorArray( \n",
    "            dtype=DTYPE, size=max_its, dynamic_size=True, clear_after_read=False, tensor_array_name='weights' \n",
    "        ).write(k, w)\n",
    "        changes = tf.TensorArray( \n",
    "            dtype=DTYPE, size=max_its, dynamic_size=True, clear_after_read=False, tensor_array_name='changes' \n",
    "        ).write( k, np.inf )\n",
    "        losses = tf.TensorArray( \n",
    "            dtype=DTYPE, size=max_its, dynamic_size=True, clear_after_read=False, tensor_array_name='losses'\n",
    "        ).write( k, loss(w) )\n",
    "        etas = tf.TensorArray( \n",
    "            dtype=DTYPE, size=max_its, dynamic_size=True, clear_after_read=False, tensor_array_name='etas'\n",
    "            \n",
    "        ).write( k, eta_0 )\n",
    "        max_grads = tf.TensorArray( \n",
    "            dtype=DTYPE, size=max_its, dynamic_size=True, clear_after_read=False, tensor_array_name='max_grads'\n",
    "        ).write( k, EPS )\n",
    "        k, weights, changes, losses, etas, max_grads = tf.while_loop( \n",
    "            cond=_continue_cond, \n",
    "            body=_eg_step, \n",
    "            loop_vars=[k, weights, changes, losses, etas, max_grads ],            \n",
    "            parallel_iterations=1,\n",
    "            name='while_loop',\n",
    "        )     \n",
    "        return  k, weights, changes, losses, etas, max_grads\n",
    "    \n",
    "\n",
    "# test EG on a trivial problem\n",
    "#################################################################\n",
    "N = 100\n",
    "q = tf.Variable(name='weights', initial_value=np.ones(N),  dtype=DTYPE)\n",
    "c = tf.constant( np.arange(N), dtype=DTYPE)\n",
    "eta_0 = tf.Variable(name='eta0', initial_value=0.05,  dtype=DTYPE)\n",
    "\n",
    "def loss( q ):\n",
    "    '''return the loss function for a given set of parameters'''\n",
    "    return tf.reduce_sum( q * c )\n",
    "\n",
    "k, weights, changes, losses, etas, max_grads = eg(loss, q, eta_0=eta_0)\n",
    "\n",
    "\n",
    "tb_writer = tf.summary.FileWriter( '/home/ubuntu/deep-learning/data/otb', graph=sess.graph )\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(k.eval(), changes.read(k).eval(), '\\n', weights.read(k).eval())\n",
    "\n",
    "subplot(221)\n",
    "plot( etas.stack().eval() )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solve the discrepancy problem\n",
    "####################################################\n",
    "%matplotlib nbagg\n",
    "from numba import jit\n",
    "from numpy.random import multinomial\n",
    "from pandas import *\n",
    "\n",
    "# generate the data from a markov process\n",
    "T = 10000\n",
    "N = 100\n",
    "R = 100\n",
    "\n",
    "@jit(nopython=True)\n",
    "def markov_sample( M, T=T, betas=betas, seed=1 ):\n",
    "    beta_seq = np.zeros( (T, N) )    \n",
    "    # initial state\n",
    "    r = multinomial( 1, np.ones(R)/R ).nonzero()[0][0]  \n",
    "    rs = np.empty(T)\n",
    "    # sample from the markov probability\n",
    "    for t in range(T):      \n",
    "        r = multinomial(1, pvals=M[r, :] ).nonzero()[0][0] \n",
    "        beta_seq[t,:] = betas[r, :]\n",
    "        rs[t] = r\n",
    "    return beta_seq, rs\n",
    "\n",
    "X = np.array( [ np.sin( np.arange(T) * 2 *  f / T * pi ) for f in arange(N) ]).T\n",
    "betas = randn( R, N )\n",
    "a = 0.01\n",
    "M = (1-a) *np.eye(R) * (R-1)/R + a * np.ones( (R,R) ) / R\n",
    "M = M / M.sum( axis=1)\n",
    "beta_seq, rs =  markov_sample(M )\n",
    "yorg = (X * beta_seq).sum(axis=1)\n",
    "y = yorg + 0.1 * randn(T)\n",
    "\n",
    "if False:\n",
    "    figure()\n",
    "    subplot(121)\n",
    "    imshow(beta_seq, aspect='auto')\n",
    "    subplot(122)\n",
    "    plot(rs, alpha=0.1)   \n",
    "\n",
    "X_trg, y_trg = X[0:T//2], y[0:T//2]\n",
    "X_tst, y_tst = X[T//2:], y[T//2:]\n",
    "\n",
    "# solve using OLS\n",
    "from statsmodels.api import OLS\n",
    "ols = OLS( exog=X_trg, endog=y_trg).fit()\n",
    "print('trg corr', Series( ols.predict( X_trg )).corr(Series(y_trg)) , 'tst corr', Series( ols.predict( X_tst)).corr(Series(y_tst) ) )\n",
    "\n",
    "\n",
    "# solve using discrpenacy\n",
    "# \\min_beta_T |sum_t q_t ||y_t - X_t \\beta_T||^2  + lambda ||beta_T||^2\n",
    "# where q_t = \\argmin_q | \\sum_t q_t L_t(\\beta_T) - \\sum_s L_s(\\beta_s) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "multinomial( 1, np.ones(R)/R).nonzero()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.squeeze(multinomial( 1, np.ones(R)/R, size=1)).nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "imshow(X, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Out[15][0].eval()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
