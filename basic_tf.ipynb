{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# load import\n",
    "\n",
    "import tensorflow as tf\n",
    "from pylab import *\n",
    "from pandas import *\n",
    "import keras as K\n",
    "import keras.backend as KB\n",
    "import os, sys, time, re\n",
    "from imp import reload\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "print(tf.__version__, sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INITIAL_LEARNING_RATE\n",
    "NUM_STEPS_PER_DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing my fjnn wrappers\n",
    "####################################################################\n",
    "\n",
    "%matplotlib nbagg\n",
    "from imp import reload\n",
    "import tensorflow as tf\n",
    "from pylab import *\n",
    "from pandas import *\n",
    "from statsmodels.api import OLS\n",
    "import convolutions as conv; reload(conv)\n",
    "import fjnn\n",
    "from fjnn._defaults import *\n",
    "import fjnn.units as fjnu; reload(fjnu)\n",
    "import fjnn.core as fjnc; reload(fjnc)\n",
    "import time, shutil, os, sys, re\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 1256\n",
    "n_epochs = 1000\n",
    "train_dir = '/home/ubuntu/deep-learning/data/test_fjnn/'\n",
    "for f in os.listdir(train_dir):\n",
    "    os.remove(train_dir+f)\n",
    "    \n",
    "w = randn(300)\n",
    "def generate_data(X):\n",
    "    return np.reshape( X, (-1, 300) ) @ w\n",
    "\n",
    "# -validation data --\n",
    "X_val = randn( 2000, 10, 30 )\n",
    "Y_val = generate_data(X_val)\n",
    "\n",
    "# -- deep learning model  --\n",
    "g = tf.Graph().as_default()\n",
    "\n",
    "def model(x, scope='model', reuse=False):\n",
    "    with tf.variable_scope(scope, reuse=reuse) as scope:\n",
    "        dense1 = fjnu.dense('dense1', x, nout=5, bias=False, W_l2=0.0, activation=tf.nn.relu)\n",
    "        dense2 = fjnu.dense('dense2', dense1, nout=2, bias=False, activation=tf.nn.relu)\n",
    "        dense3 = fjnu.dense('dense3', dense2, nout=1, bias=False, activation=tf.identity)\n",
    "        print(dense1, dense2, dense3)\n",
    "    return dense3, scope\n",
    "\n",
    "# training model\n",
    "x_trg = tf.placeholder(dtype=DTYPE, shape=(None, 10, 30), name='x_trg')\n",
    "y_trg = tf.placeholder(dtype=DTYPE, shape=(None, ), name='y_trg')\n",
    "yhat_trg, model_scope = model(x_trg)\n",
    "trg_error = fjnu.mse(yhat_trg, y_trg, name='trg_error')\n",
    "tf.add_to_collection( LOSSES, trg_error )\n",
    "trg_loss = tf.add( tf.add_n( tf.get_collection( REGULARIZERS ) ), trg_error, name='trg_loss' )\n",
    "tf.add_to_collection( LOSSES, trg_loss )\n",
    "\n",
    "# validation model\n",
    "x_val = tf.placeholder(dtype=DTYPE, shape=(None, 10, 30), name='x_val')\n",
    "y_val = tf.placeholder(dtype=DTYPE, shape=(None, ), name='y_val')\n",
    "yhat_val, model_scope = model(x_val, scope=model_scope, reuse=True)\n",
    "val_error = fjnu.mse(yhat_val, y_val, name='val_error')\n",
    "tf.add_to_collection( LOSSES, val_error)\n",
    "\n",
    "# summarize shit\n",
    "fjnc.summarize_tensors( tf.trainable_variables(), summary='variable' )\n",
    "fjnc.summarize_tensors( tf.get_collection(ACTIVATIONS), summary='activation' )\n",
    "fjnc.summarize_tensor( tf.global_norm(tf.trainable_variables(), name='trainable_variables'),  summary='training' )\n",
    "# create the loss summarizer\n",
    "loss_ema_op = fjnc.summarize_losses()\n",
    "\n",
    "#optimization varaibles\n",
    "global_step = tf.contrib.framework.create_global_step()\n",
    "# optimizer with decaying learning rate\n",
    "lr = tf.train.exponential_decay( INITIAL_LEARNING_RATE, global_step, NUM_STEPS_PER_DECAY, LEARNING_RATE_DECAY_FACTOR, staircase=True, name='learning_rate' )\n",
    "fjnc.summarize_tensor(lr, 'training')\n",
    "opt = tf.train.AdamOptimizer(0.1)\n",
    "# create a variable ema op\n",
    "variable_ema_op = fjnc.ema_variables( global_step )\n",
    "# create the descent op\n",
    "with tf.control_dependencies([loss_ema_op, variable_ema_op]):\n",
    "    train_op = fjnc.minimize(opt, trg_loss, global_step=global_step, )\n",
    "\n",
    "# start the optimization\n",
    "saver = tf.train.Saver( tf.all_variables() )\n",
    "summary_op = tf.merge_all_summaries( )\n",
    "with tf.Session() as sess:\n",
    "    sess.run( tf.initialize_all_variables() )\n",
    "    summary_writer = tf.train.SummaryWriter( train_dir, sess.graph )\n",
    "    # mini=batch sgd\n",
    "    for n in range(n_epochs):\n",
    "        X_trg = randn( batch_size, 10, 30 )\n",
    "        Y_trg = generate_data( X_trg )\n",
    "        feed_dict = { x_trg: X_trg, y_trg: Y_trg, x_val: X_val, y_val: Y_val }\n",
    "        _ = sess.run( train_op, feed_dict=feed_dict )\n",
    "        if n % 10 == 0:\n",
    "            summary_writer.add_summary( summary_op.eval( feed_dict=feed_dict ), global_step.eval() )\n",
    "        if (n*10) / n_epochs == (n*10) // n_epochs:\n",
    "            [trg_loss_value, trg_err_value, val_err_value] = sess.run( [trg_loss, trg_error, val_error], feed_dict=feed_dict)\n",
    "            print( n, trg_loss_value, trg_err_value, val_err_value )\n",
    "    Yhat_val = yhat_val.eval( feed_dict=feed_dict )\n",
    "    \n",
    "\n",
    "%matplotlib nbagg\n",
    "plot( Yhat_val, Y_val, '.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?os.remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.get_collection(WEIGHTS)[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(tf.get_collection(WEIGHTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "reload(fjnu)\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.Session() as sess:\n",
    "        w = tf.Variable( tf.random_normal((5, 3)) )\n",
    "        print( tf.nn.l2_loss( tf.matmul(tf.transpose(w), w) ) )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing linear deconvoluation with tensorflow\n",
    "#####################################################\n",
    "%matplotlib nbagg\n",
    "from imp import reload\n",
    "import tensorflow as tf\n",
    "from pylab import *\n",
    "from pandas import *\n",
    "from statsmodels.api import OLS\n",
    "import convolutions as conv; reload(conv)\n",
    "import fjnn\n",
    "import fjnn.units as fjnu; reload(fjnu)\n",
    "import fjnn.core; reload(fjnn.core)\n",
    "import time, shutil, os, sys\n",
    "\n",
    "N = 5000;\n",
    "P = 5;\n",
    "H = 100;\n",
    "dtype = tf.float32;\n",
    "df, conv_kernels = conv.get_data(N, P, H)\n",
    "X = conv.build_hankel_tensors(df, P, H)\n",
    "Y = df['yl'].values\n",
    "train_dir = '/home/ubuntu/deep-learning/data/linear_deconv_tf'\n",
    "shutil.rmtree(train_dir)\n",
    "max_steps = 5000;\n",
    " \n",
    "with tf.Graph().as_default():\n",
    "    #the training set\n",
    "    x = tf.placeholder(shape=(None, P, H), dtype=dtype, name='trg_input')\n",
    "    y = tf.placeholder(shape=(None, ), dtype=dtype)\n",
    "    with tf.variable_scope('linear_deconv') as model:        \n",
    "        yhat = fjnu.dense(x, num_output=1, bias=False, W_l2=0.1)\n",
    "        trg_loss = fjnu.mse(y, yhat, name='trg_loss')        \n",
    "    tf.add_to_collection('trg_losses', trg_loss)\n",
    "    \n",
    "    # the validation set\n",
    "    x_val = tf.placeholder(shape=(None, P, H), dtype=dtype, name='val_input')\n",
    "    y_val = tf.placeholder(shape=(None, ), dtype=dtype)\n",
    "    with tf.variable_scope(model, reuse=True):\n",
    "        yhat_val = fjnn.units.dense(x_val, num_output=1, bias=False)\n",
    "    val_loss = fjnu.mse(y_val, yhat_val, name='val_loss')        \n",
    "    tf.add_to_collection('val_losses', val_loss)\n",
    "        \n",
    "    print('trainable vars', [(v.name, v.get_shape()) for v in tf.trainable_variables() ])\n",
    "    print('training losses', [(v.name, v.get_shape()) for v in tf.get_collection('trg_losses') ])\n",
    "    print('validation losses', [(v.name, v.get_shape()) for v in tf.get_collection('val_losses') ])\n",
    "    print( 'yhat shapes', y_val.get_shape(), yhat_val.get_shape(), ((y_val - tf.squeeze(yhat_val)) **2).get_shape() )\n",
    "    cost_ema_op = fjnn.core.summarize_costs()\n",
    "    \n",
    "    #Train the model\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    var_ema_op = fjnn.core.ema_variables(global_step)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.1, epsilon=)        \n",
    "    with tf.control_dependencies([var_ema_op, cost_ema_op]):\n",
    "        train_ops = fjnn.core.build_train_ops(opt, global_step, validation=True)\n",
    "\n",
    "    # set up the various summarizers and variable emas\n",
    "    fjnn.core.summarize_collection(tf.trainable_variables())\n",
    "    fjnn.core.summarize_collection(tf.get_collection('activations'))\n",
    "    fjnn.core.summarize_collection(tf.get_collection('gradients'), 'gradient')        \n",
    "    print(tf.get_collection('gradients'))\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver( tf.all_variables() )\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=False)) as sess:        \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        print(sess.run(trg_loss, feed_dict={x:X, y:Y}))\n",
    "        print(sess.run(val_loss, feed_dict={x_val:X, y_val:Y}))          \n",
    "        # Logging\n",
    "        summary_writer = tf.train.SummaryWriter(train_dir, sess.graph)\n",
    "        for step in range(max_steps):\n",
    "            start_time = time.time()\n",
    "            _, total_trg_cost, total_val_cost = sess.run(train_ops, feed_dict={x:X, y:Y, x_val:X, y_val:Y})                \n",
    "            if step % 100 == 0:\n",
    "                summary_str = sess.run(summary_op, feed_dict={x:X, y:Y, x_val:X, y_val:Y})\n",
    "                summary_writer.add_summary(summary_str, global_step.eval())\n",
    "                w = tf.trainable_variables()[0].eval()\n",
    "                print(global_step.eval(), total_trg_cost, total_val_cost, mean((np.squeeze((np.reshape(X, [-1, P*H])@w)) - df['yl'].values)**2) )\n",
    "        \n",
    "ols = OLS(\n",
    "    endog=df['yl'],\n",
    "    exog=np.reshape(X, [-1,P*H] )\n",
    ").fit()\n",
    "for p in range(P):        \n",
    "    subplot(221); plot( conv_kernels[p] )\n",
    "    subplot(222); plot( df['x_%d'%p] )\n",
    "subplot(223); plot( df['ynl'], '-k', alpha=0.5 )\n",
    "subplot(224)\n",
    "plot(ols.params.values, label='ols')\n",
    "\n",
    "subplot(224); plot(w, label='tf')\n",
    "gca().legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(ols.resid**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing deconvoluation with keras\n",
    "#####################################################\n",
    "%matplotlib nbagg\n",
    "\n",
    "import scipy.linalg as SLA\n",
    "from numba import jit\n",
    "import numpy as np;\n",
    "import os, sys, time\n",
    "import tensorflow as tf\n",
    "from pylab import *\n",
    "from pandas import *\n",
    "from statsmodels.api import OLS\n",
    "import convolutions as conv; reload(conv)\n",
    "\n",
    "N = 5000;\n",
    "P = 5;\n",
    "nb_epoch=50;\n",
    "batch_size=128\n",
    "\n",
    "H = 100; #the convoluation window length\n",
    "conv.linear_test(N, P, H, nb_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "\n",
    "reload(conv)\n",
    "\n",
    "df, conv_kernels = conv.get_data(N, P, H)\n",
    "X = conv.build_hankel_tensors(df, P, H)\n",
    "ols = OLS(\n",
    "    endog=df['ynl'],\n",
    "    exog=np.reshape(X, [-1,P*H] )\n",
    ").fit()\n",
    "km = conv.linear_deconv(X, df['ynl'].values, nb_epoch, batch_size)\n",
    "w = km.get_weights()[0]\n",
    "for p in range(P):        \n",
    "    subplot(221); plot( conv_kernels[p] )\n",
    "    subplot(222); plot( df['x_%d'%p] )\n",
    "subplot(223); plot( df['ynl'], '-k', alpha=0.5 )\n",
    "subplot(224)\n",
    "plot(ols.params.values, label='ols')\n",
    "\n",
    "subplot(224); plot(w, label='keras')\n",
    "gca().legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing basic convolution ideas\n",
    "# Note : np.convolve is a causal convolution\n",
    "# tf.conv2d is a symmetric inner-product - not a convolution !!!\n",
    "#######################################################\n",
    "%matplotlib nbagg\n",
    "\n",
    "import scipy.linalg as SLA\n",
    "from numba import jit\n",
    "import numpy as np;\n",
    "import os, sys, time\n",
    "import tensorflow as tf\n",
    "from pylab import *\n",
    "from pandas import *\n",
    "\n",
    "N = 5000;\n",
    "P = 5;\n",
    "H = 100;\n",
    "dtype = tf.float32;\n",
    "reg_wt = 0.001;\n",
    "# variables that affect the learning\n",
    "batch_size = 1024;\n",
    "max_steps = 5000;\n",
    "train_dir = '/home/ubuntu/fj/data/basic_conv'\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "# -- prepare the dataset  --\n",
    "def gfunc(n, sigma):\n",
    "    '''gaussian functions'''\n",
    "    return exp( -pow(linspace(-6*sigma, 6*sigma, n),2)/2/sigma**2 )/sqrt(2*pi*sigma**2)\n",
    "# the generating convolution kernels \n",
    "conv_kernels = [         \n",
    "    gfunc(H, sigma)*cos(linspace(0,2*pi*(sigma-1), H)) for sigma in range(1,2*P,2)\n",
    "]\n",
    "# the input dataframe\n",
    "df = DataFrame(\n",
    "    hstack( [\n",
    "        randn(N, 4), \n",
    "        reshape( sin(linspace(0, N/2/pi, N))>0, (N,1))\n",
    "    ] ),\n",
    "    columns=['x_%d'%j for j in range(P)]\n",
    ")\n",
    "# convolve 'same' is a causal convolution y[t] = \\sum_{i=0}^n x[t-i]h[i]\n",
    "df['y'] = sum(array([ convolve( df['x_%d'%p], conv_kernels[p], 'same') for p in range(P-1) ]), axis=0) + df['x_%d'%(P-1)] + 0.1*randn(N)\n",
    "\n",
    "if False:\n",
    "    for p in range(P):\n",
    "        plot(conv_kernels[p])\n",
    "    figure();\n",
    "    plot(df['x_0'], '--')\n",
    "    plot(df['x_1'], '--')\n",
    "    plot(df['y'], '-')\n",
    "    # -- test tensorflow convolution -- it is symmetric innerproduct not a 'convolution' --    \n",
    "    with tf.Graph().as_default() as g, tf.Session() as sess:\n",
    "        x = tf.Variable( initial_value=array([0, 1.0, 20.0, 30]), dtype=tf.float32 )\n",
    "        h = tf.Variable( initial_value=array([0.1, 0.5, 0.2, 0.2]), dtype=tf.float32 )\n",
    "        xx = tf.reshape(x, (-1, 4,1,1))\n",
    "        hh = tf.reshape(h, (4,1,1, 1))\n",
    "        sess.run(x.initializer)\n",
    "        sess.run(h.initializer)\n",
    "        tf.initialize_all_tables()\n",
    "        cc = tf.nn.conv2d(xx, hh, strides=[1,1,1,1], padding='VALID')\n",
    "        print(tf.squeeze(xx).eval(), '\\n', tf.squeeze(hh).eval(),'\\n', tf.squeeze(cc).eval())\n",
    "\n",
    "@jit(nopython=True, nogil=True)\n",
    "def hankel(x, w):\n",
    "    '''\n",
    "    Reshape the x vector into a causal hankel matrix of width w\n",
    "    op[t] = [x_t ... x_{t-w}]\n",
    "    '''\n",
    "    n = len(x)\n",
    "    op = zeros( (n, w))\n",
    "    for t in range(n):\n",
    "        for v in range(w):\n",
    "            if t - v < 0 : break;\n",
    "            op[t, v] = x[t-v]\n",
    "    return op\n",
    "\n",
    "def tensorize_x(df=df, P=P, w=H):\n",
    "    ''' convert the input columns into an [N,P,H] tensor '''\n",
    "    return np.transpose( \n",
    "        np.array( [ hankel(df['x_%d'%p].values, w) for p in range(P) ]), \n",
    "        [1,2,0] \n",
    "    )        \n",
    "\n",
    "# --- don't use tensorflow convolutions -- instead reshape the data ----\n",
    "with tf.Graph().as_default() as g , tf.Session() as sess: \n",
    "    # placeholders for the input and output variables\n",
    "    y = tf.placeholder(tf.float32, shape=N, name='y')    \n",
    "    x = tf.placeholder(tf.float32, shape=(N, H, P), name='x')    \n",
    "    x_batch, y_batch = tf.train.batch(\n",
    "        [ tensorize_x(), df['y'].values], \n",
    "        batch_size=batch_size,\n",
    "        num_threads=1,\n",
    "        enqueue_many=True\n",
    "    )\n",
    "    x = tf.cast(x_batch, dtype)\n",
    "    y = tf.cast(y_batch, dtype)\n",
    "    print('x shape', x.get_shape())    \n",
    "#     tf.scalar_summary( 'x/len', x.get_shape()[0].value )\n",
    "#     tf.histogram_summary( 'x/histogram', x )\n",
    "#     # define the name scope for the convolution layer\n",
    "#     with tf.name_scope('conv0') as scope:\n",
    "#         # create the P convolution kernels (or get it if it exists)         \n",
    "#         kernel = tf.get_variable(\n",
    "#             name='kernel', shape=[H,1,P,1], dtype=dtype,\n",
    "#             initializer=tf.contrib.layers.xavier_initializer_conv2d(uniform=False, dtype=dtype) \n",
    "#         )\n",
    "#         # apply the Hx1 convolution kernel on each of the P input channels separately \n",
    "#         # and squeeze out redudant dimensions\n",
    "#         conv = tf.squeeze( tf.nn.depthwise_conv2d(\n",
    "#                 input=tf.reshape(x, [-1,H,1,P]), filter=kernel, \n",
    "#                 strides=[1, 1, 1, 1], padding='VALID', name='conv', \n",
    "#         ), name='squeezed-conv' )\n",
    "#         # add some summaries\n",
    "#         tf.histogram_summary(kernel.name + '/activations', kernel)\n",
    "#         tf.scalar_summary(kernel.name + '/sparsity', tf.nn.zero_fraction(kernel))\n",
    "#     # -- create the output layer --\n",
    "#     with tf.name_scope('output') as scope:\n",
    "#         W = tf.ones( [P, 1])\n",
    "#         yhat = tf.matmul(conv, W, name='yhat')\n",
    "#     # -- define the loss function and regularizers --    \n",
    "#     with tf.name_scope('loss') as scope:\n",
    "#         l2error = tf.reduce_mean( tf.square(y-yhat), name='l2error')\n",
    "#         tf.add_to_collection('losses', l2error)\n",
    "#         # create a \"differencing\" regularization matrix\n",
    "#         regmtx = tf.constant(value=SLA.circulant([1,-1]+[0]*(H-2))[1:], dtype=dtype )\n",
    "#         l2reg = tf.nn.l2_loss( tf.matmul( regmtx, tf.squeeze(kernel) ), name='l2reg') ;\n",
    "#         tf.add_to_collection('losses', l2reg)\n",
    "#         total_loss = l2error + l2reg*reg_wt;    \n",
    "#         # add some summaries\n",
    "#         tf.scalar_summary( total_loss.op.name, total_loss )\n",
    "#         tf.scalar_summary( l2reg.op.name, l2reg )\n",
    "#         tf.scalar_summary( l2error.op.name, l2error )\n",
    "#     # -- define a training step --\n",
    "#     train_step = tf.train.AdadeltaOptimizer().minimize( total_loss )            \n",
    "#     # -- actually run the model --\n",
    "#     # Create a saver.\n",
    "#     saver = tf.train.Saver( tf.all_variables() )\n",
    "#     # Build the summary operation based on the TF collection of Summaries.\n",
    "#     summary_op = tf.merge_all_summaries()\n",
    "#     # Initialization operation\n",
    "#     sess.run(tf.initialize_all_variables())\n",
    "#     # create a coordinator to manage the threads\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     # Start the queue runners (created in tf.train.batch)-> to read the input\n",
    "#     threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "#     # Logging\n",
    "#     summary_writer = tf.train.SummaryWriter(train_dir, sess.graph)    \n",
    "#     try:\n",
    "#         for step in range(max_steps):\n",
    "#             # has a thread signalled stopping ?\n",
    "#             if coord.should_stop():\n",
    "#                 break;\n",
    "#             train_step.run()\n",
    "#             start_time = time.time()\n",
    "#             loss_value = sess.run(total_loss)\n",
    "#             duration = time.time() - start_time\n",
    "#             assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "#             if step % 100 == 0:\n",
    "#                 num_examples_per_step = batch_size\n",
    "#                 examples_per_sec = num_examples_per_step / duration\n",
    "#                 sec_per_batch = float(duration)\n",
    "#                 format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "#                 print (format_str%(datetime.now(), step, loss_value, examples_per_sec, sec_per_batch))\n",
    "#             if step % 500 == 0:\n",
    "#                 summary_writer.add_summary(sess.run(summary_op), step)            \n",
    "#     except tf.errors.OutOfRangeError:\n",
    "#         print('Done training -- epoch limit reached')\n",
    "#     finally:\n",
    "#         # When done, ask the threads to stop.\n",
    "#         coord.request_stop()\n",
    "#     # Wait for threads to finish.\n",
    "#     coord.join(threads)\n",
    "#     conv_filter = kernel.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tensorflow Conv nets (cifar10) tutorial\n",
    "########################################################\n",
    "from tensorflow.models.image.cifar10 import cifar10, cifar10_input\n",
    "import fj_cifar10\n",
    "reload(fj_cifar10)\n",
    "# tf.reset_default_graph()\n",
    "images = tf.placeholder(tf.float32, [fj_cifar10.batch_size,32,32,3])\n",
    "labels = tf.placeholder(tf.int64, [fj_cifar10.batch_size])\n",
    "\n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    # 64-channels of 5x5x3 convolution kernels.\n",
    "    kernel = fj_cifar10._variable_with_weight_decay('weights', shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0)\n",
    "    # stride = 1, i.e no downsampling.\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = fj_cifar10._variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    # shape is [batch, height, width, 64]\n",
    "    conv1 = tf.nn.relu(bias, name=scope.name)\n",
    "    fj_cifar10._activation_summary(conv1)\n",
    "# -- pool1 : pooling of 3x3 for each channel, with downsampling factor of 2.\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "# -- norm1: local-response normalization --\n",
    "# sqr_sum[a, b, c, d] =  sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\n",
    "# output = input / (bias + alpha * sqr_sum) ** beta\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "# -- conv2 --\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    # 64channels of 5x5x64 convolution kernels\n",
    "    kernel = fj_cifar10._variable_with_weight_decay('weights', shape=[5, 5, 64, 64], stddev=5e-2, wd=0.0)\n",
    "    # no downsampling. shape=[batch, height, width, 64]\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = fj_cifar10._variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(bias, name=scope.name)\n",
    "    fj_cifar10._activation_summary(conv2)\n",
    "# -- norm2 --\n",
    "norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "# -- pool2: 3x3 pooling and downsample by 2\n",
    "pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "# -- local3: fully connected --\n",
    "with tf.variable_scope('local3') as scope:\n",
    "    # Reshape the tensor - so we can perform a single matrix multiply.\n",
    "    reshape = tf.reshape(pool2, [fj_cifar10.batch_size, -1])\n",
    "    dim = reshape.get_shape()[1].value #should be width/2/2 x height/2/2 x 64 \n",
    "    weights = fj_cifar10._variable_with_weight_decay('weights', shape=[dim, 384], stddev=0.04, wd=0.004)\n",
    "    biases = fj_cifar10._variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    fj_cifar10._activation_summary(local3)\n",
    "# --  local4 --\n",
    "with tf.variable_scope('local4') as scope:\n",
    "    weights = fj_cifar10._variable_with_weight_decay('weights', shape=[384, 192], stddev=0.04, wd=0.004)\n",
    "    biases = fj_cifar10._variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    # batch_size x 192\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    fj_cifar10._activation_summary(local4)\n",
    "# softmax, i.e. softmax(WX + b) -> still in logits. exp is not applied.\n",
    "with tf.variable_scope('softmax_logits') as scope:\n",
    "    weights = fj_cifar10._variable_with_weight_decay('weights', [192, fj_cifar10.NUM_CLASSES], stddev=1/192.0, wd=0.0)\n",
    "    biases = fj_cifar10._variable_on_cpu('biases', [fj_cifar10.NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "    # shape = batch_size x NUM_CLASSES\n",
    "    softmax_logits = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    fj_cifar10._activation_summary(softmax_logits)\n",
    "\n",
    "    \n",
    "#converts labels into 1-hot vectors. ent[x,y] = \\sum_i y_i logit_i - log(\\sum_k logit_k)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(softmax_logits, labels, name='cross_entropy_per_example')\n",
    "# reduce along training dimension\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')    \n",
    "tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "# for k, v in locals().items():\n",
    "#     if type(v) is tf.python.framework.ops.Tensor:\n",
    "#         print (k, v.name, v.get_shape())\n",
    "g = tf.get_default_graph()\n",
    "for k in g.get_all_collection_keys():\n",
    "    for v in g.get_collection(k):\n",
    "        print (k, v.name, v.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MNIST conv NN\n",
    "###############################################################\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.Graph().as_default() as g , tf.Session() as sess: \n",
    "\n",
    "    # the input and output placeholders\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "    # first layer. The first two dimensions are the patch size, the next is the number of input channels, and the last is the number of output channels.\n",
    "    W_conv1 = weight_variable( [5, 5, 1, 32] )\n",
    "    b_conv1 = bias_variable( [32] )\n",
    "\n",
    "    #reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of input channels.\n",
    "    x_image = tf.reshape(x, [-1, 28, 28,1])\n",
    "\n",
    "    # convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool.\n",
    "    h_conv1 = tf.nn.relu( conv2d( x_image, W_conv1 ) + b_conv1 )\n",
    "    h_pool1 = max_pool_2x2( h_conv1 )\n",
    "\n",
    "    #The second layer will have 64 features for each 5x5 patch.\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # reshape operates with the right most index as the fastest (i.e. it fills by counting from right to left). Therefore, it makes sense to put the sample index along the first index.\n",
    "\n",
    "    # Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons to allow processing on the entire image. \n",
    "    # We reshape the tensor from the pooling layer into a batch of vectors, multiply by a weight matrix, add a bias, and apply a ReLU.\n",
    "    W_fc1 = weight_variable( [7 * 7 * 64, 1024] )\n",
    "    b_fc1 = bias_variable( [1024] )\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu( tf.matmul(h_pool2_flat, W_fc1) + b_fc1 )\n",
    "\n",
    "    # We create a placeholder for the probability that a neuron's output is kept during dropout. \n",
    "    # This allows us to turn dropout on during training, and turn it off during testing. \n",
    "    # TensorFlow's tf.nn.dropout op automatically handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling.1\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "    # Finally, we add a softmax layer, just like for the one layer softmax regression above.\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    y_conv = tf.nn.softmax( tf.matmul(h_fc1_drop, W_fc2) + b_fc2 )\n",
    "\n",
    "    cross_entropy = tf.reduce_mean( -tf.reduce_sum( y_ * tf.log(y_conv), reduction_indices=[1] ) )\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize( cross_entropy )\n",
    "    correct_prediction = tf.equal( tf.argmax(y_conv, 1), tf.argmax(y_, 1) )\n",
    "    accuracy = tf.reduce_mean( tf.cast(correct_prediction, tf.float32) )\n",
    "    sess.run( tf.initialize_all_variables() )\n",
    "    \n",
    "\n",
    "    trg_acc = [];\n",
    "    tst_acc = [];\n",
    "    idx = [];\n",
    "    for i in range(20000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval( feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0} )\n",
    "            test_accuracy = accuracy.eval( feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0} )\n",
    "            print(\"%d | training=%g, testing=%g\"%(i, train_accuracy, test_accuracy))\n",
    "            trg_acc.append(train_accuracy)\n",
    "            tst_acc.append(test_accuracy)\n",
    "            idx.append(i)\n",
    "        train_step.run( feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5} )\n",
    "\n",
    "    print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "    %matplotlib nbagg\n",
    "    plot( idx, trg_acc, '--k')\n",
    "    plot( idx, tst_acc, '-b')\n",
    "    gca().set_ylim([0.8,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow mechanics\n",
    "################################################################\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "NUM_CLASSES = 10 # The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "IMAGE_SIZE = 28 # The MNIST images are always 28x28 pixels.\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "\n",
    "# Basic model parameters \n",
    "flags = dict(\n",
    "    learning_rate=0.01, # Initial learning rate\n",
    "    max_steps=2000, #'Number of steps to run trainer\n",
    "    hidden1=128, #Number of units in hidden layer 1.\n",
    "    hidden2=2, #'Number of units in hidden layer 2.\n",
    "    batch_size=100, #'Batch size.  Must divide evenly into the dataset sizes.\n",
    "    train_dir='/home/ubuntu/fj/deep-learning/code/MNIST_data/', #Directory to put the training data\n",
    "    fake_data=False, #If true, uses fake data for unit testing.\n",
    ")\n",
    "\n",
    "def placeholder_inputs(batch_size=None):\n",
    "    # Note that the shapes of the placeholders match the shapes of the full image and label tensors, \n",
    "    # except that the first dimension is now batch_size\n",
    "    images_placeholder = tf.placeholder( tf.float32, shape=(batch_size, IMAGE_PIXELS) )\n",
    "    labels_placeholder = tf.placeholder( tf.int32, shape=(batch_size) )\n",
    "    return images_placeholder, labels_placeholder\n",
    "\n",
    "def inference(images, hidden1_units, hidden2_units):\n",
    "    # Hidden layer 1\n",
    "    with tf.name_scope('hidden1') as scope:\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal( [IMAGE_PIXELS, hidden1_units], stddev=1.0 / math.sqrt(float(IMAGE_PIXELS)) ),\n",
    "            name='weights' )\n",
    "        biases = tf.Variable( tf.zeros([hidden1_units]), name='biases' )\n",
    "        hidden1 = tf.nn.relu( tf.matmul(images, weights) + biases, name=scope )\n",
    "    # Hidden layer 2\n",
    "    with tf.name_scope('hidden2') as scope:\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal( [hidden1_units, hidden2_units], stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights' )\n",
    "        biases = tf.Variable( tf.zeros([hidden2_units]), name='biases' )\n",
    "        hidden2 = tf.nn.relu( tf.matmul(hidden1, weights) + biases, name=scope)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal( [hidden2_units, NUM_CLASSES], stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable( tf.zeros([NUM_CLASSES]), name='biases' )\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "    return logits #exp(logits_i)/sum_j exp(logits_j) = p_i\n",
    "\n",
    "def lossfunction(logits, labels):\n",
    "    labels = tf.to_int64( labels )\n",
    "    #automatically produce 1-hot labels from the labels_placeholder and coverts logits into (log) probabilities.\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits( logits, labels, name='xentropy' )\n",
    "    # average the cross entropy values across the batch dimension\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    return loss\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    # tf.scalar_summary is an op for generating summary values into the events file when used with a SummaryWriter. \n",
    "    # it will emit the snapshot value of the loss every time summaries are written out.\n",
    "    tf.scalar_summary( loss.op.name, loss )\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step. #note the trainable=False !!\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\"\"\"\n",
    "    # For a classifier model, we can use the in_top_k Op.\n",
    "    # It returns a bool tensor with shape [batch_size] that is true for\n",
    "    # the examples where the label is in the top k (here k=1) of all logits for that example.\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    # Return the number of true entries.\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "\n",
    "def get_feed_dict(data_set, images_pl, labels_pl):\n",
    "    images_feed, labels_feed = data_set.next_batch( flags['batch_size'], flags['fake_data'] )\n",
    "    feed_dict = { images_pl: images_feed, labels_pl: tf.arg_max(labels_feed, 1).eval()}\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(sess, eval_correct, images_placeholder, labels_placeholder, data_set):\n",
    "    \"\"\"Evaluate against the full epoch of data. \"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples // flags['batch_size']\n",
    "    num_examples = steps_per_epoch * flags['batch_size']\n",
    "    for step in range(steps_per_epoch):\n",
    "        feed_dict = get_feed_dict(data_set, images_placeholder, labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "    precision = true_count / num_examples\n",
    "    print( '  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision) )\n",
    "\n",
    "# force gpu placement\n",
    "with tf.device('/gpu:1'):\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "        # Generate placeholders for the images and labels.\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs( flags['batch_size'] )\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        logits = inference(images_placeholder, flags['hidden1'], flags['hidden2'])\n",
    "        # Add to the Graph the Ops for loss calculation.\n",
    "        loss = lossfunction(logits, labels_placeholder)\n",
    "        # Add to the Graph the Ops that calculate and apply gradients.\n",
    "        train_op = training(loss, flags['learning_rate'])\n",
    "        # Add the Op to compare the logits to the labels during evaluation.\n",
    "        eval_correct = evaluation(logits, labels_placeholder)\n",
    "        # Build the summary operation based on the TF collection of Summaries.\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        # Create a saver for writing training checkpoints.\n",
    "        saver = tf.train.Saver()\n",
    "        # Create a session for running Ops on the Graph.    \n",
    "        with tf.Session(  ) as sess:\n",
    "            # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "            summary_writer = tf.train.SummaryWriter( flags['train_dir'], sess.graph )\n",
    "            # And then after everything is built, run the Op to initialize the variables.\n",
    "            sess.run( tf.initialize_all_variables() )\n",
    "            # Start the training loop.\n",
    "            for step in range( flags['max_steps'] ):\n",
    "                start_time = time.time()\n",
    "                # Fill a feed dictionary with the actual set of images and labels for this particular training step.\n",
    "                feed_dict = get_feed_dict(mnist.train, images_placeholder, labels_placeholder)\n",
    "                # Run one step of the model.  The return values are the activations from the `train_op` \n",
    "                # (which is discarded) and the `loss` Op.  To inspect the values of your Ops or variables, \n",
    "                # you may inclutde them in the list passed to sess.run() and the value tensors will be\n",
    "                # returned in the tuple from the call.\n",
    "                _, loss_value = sess.run( [train_op, loss], feed_dict=feed_dict)\n",
    "                duration = time.time() - start_time\n",
    "                # Write the summaries and print an overview fairly often.\n",
    "                if step % 100 == 0:\n",
    "                    # Print status to stdout.\n",
    "                    print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                    # Update the events file.\n",
    "                    summary_str = sess.run( summary_op, feed_dict=feed_dict )\n",
    "                    summary_writer.add_summary( summary_str, step )\n",
    "                    summary_writer.flush()\n",
    "                # Save a checkpoint and evaluate the model periodically.\n",
    "                if (step + 1) % 1000 == 0 or (step + 1) == flags['max_steps']:\n",
    "                    checkpoint_file = os.path.join( flags['train_dir'], 'checkpoint' )\n",
    "                    saver.save( sess, checkpoint_file, global_step=step)\n",
    "                    # Evaluate against the training set.\n",
    "                    print('Training Data Eval:')\n",
    "                    evaluate(sess, eval_correct, images_placeholder, labels_placeholder, mnist.train)\n",
    "                    # Evaluate against the validation set.\n",
    "                    print('Validation Data Eval:')\n",
    "                    evaluate(sess, eval_correct, images_placeholder, labels_placeholder, mnist.validation)\n",
    "                    # Evaluate against the test set.\n",
    "                    print('Test Data Eval:')\n",
    "                    evaluate(sess, eval_correct, images_placeholder, labels_placeholder, mnist.test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow contrib logging tutorial\n",
    "####################################################################################\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Data sets\n",
    "IRIS_TRAINING = \"/home/ubuntu/fj/data/iris_training.csv\"\n",
    "IRIS_TEST = \"/home/ubuntu/fj/data/iris_test.csv\"\n",
    "\n",
    "# Load datasets.\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING,\n",
    "                                                       target_dtype=np.int)\n",
    "test_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TEST,\n",
    "                                                   target_dtype=np.int)\n",
    "\n",
    "# Specify that all features have real-value data\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n",
    "                                            hidden_units=[10, 20, 10],\n",
    "                                            n_classes=3,\n",
    "                                            model_dir=\"/tmp/iris_model\")\n",
    "\n",
    "# Fit model.\n",
    "classifier.fit(x=training_set.data,\n",
    "               y=training_set.target,\n",
    "               steps=2000)\n",
    "\n",
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(x=test_set.data,\n",
    "                                     y=test_set.target)[\"accuracy\"]\n",
    "print('Accuracy: {0:f}'.format(accuracy_score))\n",
    "\n",
    "# Classify two new flower samples.\n",
    "new_samples = np.array(\n",
    "    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\n",
    "y = classifier.predict(new_samples)\n",
    "print('Predictions: {}'.format(str(y)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
