{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# start a tensorflow session\n",
    "from pylab import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "DTYPE = tf.float32\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "g = tf.Graph().as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-33-7c154f86ec0e>:71 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-33-7c154f86ec0e>:71 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652233 \n",
      " [  6.05241776e-01   2.38923907e-01   9.43173319e-02   3.72325554e-02\n",
      "   1.46979075e-02   5.80213312e-03   2.29044212e-03   9.04172543e-04\n",
      "   3.56930279e-04   1.40901248e-04   5.56220439e-05   2.19572739e-05\n",
      "   8.66783193e-06   3.42170324e-06   1.35074799e-06   5.33219747e-07\n",
      "   2.10493369e-07   8.30940863e-08   3.28021130e-08   1.29489308e-08\n",
      "   5.11170439e-09   2.01789097e-09   7.96578692e-10   3.14456877e-10\n",
      "   1.24134772e-10   4.90032702e-11   1.93444739e-11   7.63639846e-12\n",
      "   3.01453784e-12   1.19001597e-12   4.69768755e-13   1.85445463e-13\n",
      "   7.32061980e-14   2.88988194e-14   1.14080642e-14   4.50342812e-15\n",
      "   1.77776906e-15   7.01790620e-16   2.77037901e-16   1.09363249e-16\n",
      "   4.31720749e-17   1.70425726e-17   6.72769815e-18   2.65581949e-18\n",
      "   1.04840831e-18   4.13868394e-19   1.63378407e-19   6.44950333e-20\n",
      "   2.54599956e-20   1.00505546e-20   3.96754434e-21   1.56622337e-21\n",
      "   6.18280602e-22   2.44071772e-22   9.63494767e-23   3.80348339e-23\n",
      "   1.50145679e-23   5.92713836e-24   2.33979063e-24   9.23653075e-25\n",
      "   3.64620163e-25   1.43937040e-25   5.68204061e-26   2.24303708e-26\n",
      "   8.85458310e-27   3.49542495e-27   1.37985071e-27   5.44707781e-28\n",
      "   2.15028369e-28   8.48843211e-29   3.35088097e-29   1.32279218e-29\n",
      "   5.22182802e-30   2.06136495e-30   8.13742077e-31   3.21231996e-31\n",
      "   1.26809133e-31   5.00590676e-32   1.97612341e-32   7.80093022e-33\n",
      "   3.07948241e-33   1.21565346e-33   4.79890150e-34   1.89440902e-34\n",
      "   7.47833722e-35   2.95214501e-35   1.16538434e-35   4.60046976e-36\n",
      "   1.81607025e-36   7.16910079e-37   2.83006933e-37   1.11719327e-37\n",
      "   4.41022734e-38   1.74097770e-38   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# the EG algorithsm\n",
    "def eg( loss, w, shift_rate=tf.constant(0, dtype=DTYPE), eta_0=None, max_its=100, tol=tf.constant(1e-6), step_rule='sqrt' ):\n",
    "    '''\n",
    "    do batch eg on the given loss tensor. w are the simplex weights\n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "    weights: tf.Tensor\n",
    "    relative change in weights: tf.Tensor\n",
    "    iteration count: tf.Tensor\n",
    "    max grad: tf.Tensor\n",
    "    losses: tf.TensorArray of losses to date\n",
    "    '''    \n",
    "    \n",
    "    def _eg_step( w, chg_w, k, G_inf, losses ):\n",
    "        '''\n",
    "        Parameters\n",
    "        --------------\n",
    "        losses: tensor of losses to date\n",
    "        \n",
    "        eg step with no sleeping expert correction\n",
    "        # for sleeping expert do this \n",
    "        # awake_sum = tf.reduce_sum( tf.where( nan_mask, tf.zeros_like(w), w) )\n",
    "        # w_s = tf.where( nan_mask, tf.zeros_like(w), tf.divide( w, awake_sum ) )\n",
    "        \n",
    "        # theoretical optimal eta = sqrt( 2 log N / K ) / G_inf\n",
    "        '''\n",
    "        grad = tf.gradients( loss(w), w )[0]\n",
    "        nan_mask = tf.is_nan( grad )        \n",
    "        grad = tf.where( nan_mask, tf.zeros_like(grad, dtype=DTYPE), grad )\n",
    "        # find the new max gradient\n",
    "        G_inf = tf.maximum( G_inf, tf.reduce_max( tf.abs( grad ) ) )        \n",
    "        # pre-condition\n",
    "        grad = grad - tf.reduce_min( grad )             \n",
    "        if step_rule == 'constant':\n",
    "            eta = eta_0 / G_inf\n",
    "        elif step_rule == 'sqrt':\n",
    "            eta = eta_0 / tf.sqrt( 1 +  tf.cast( k, dtype=DTYPE) )\n",
    "        elif step_rule == 'inv':\n",
    "            eta = eta_0 / ( 1 + tf.cast( k, dtype=DTYPE) )\n",
    "        w_n = w * tf.exp( -eta  * grad )\n",
    "        w_n = w_n / tf.reduce_sum( w_n ) * (1 - shift_rate) + shift_rate     \n",
    "        chg_w = tf.reduce_sum( tf.abs(w_n - w) ) / tf.reduce_sum( tf.abs( w ))                \n",
    "        with tf.control_dependencies( [tf.Print( eta, [k, eta, loss(w_n), chg_w], '\\tk, eta, loss(w), chg_w = ' )] ):            \n",
    "            return w_n, chg_w, k + 1, G_inf, losses.write( k, loss(w_n) )\n",
    "        \n",
    "    def _continue_cond( w, chg_w, k, G_inf, losses):\n",
    "        return tf.logical_not( tf.logical_or( k >= max_its, chg_w < tol  ) )\n",
    "    \n",
    "    if eta_0 is None:\n",
    "        eta_0 = tf.constant( np.sqrt( 2 * np.log( w.get_shape().as_list()[0] ) / max_its), dtype=DTYPE  )         \n",
    "    chg_w = tf.constant( np.inf, dtype=DTYPE )\n",
    "    k = tf.constant( 0, dtype=tf.int32 )\n",
    "    G_inf = tf.constant(-np.inf, dtype=DTYPE)\n",
    "    losses = tf.TensorArray( dtype=DTYPE, size=max_its, dynamic_size=True, clear_after_read=False, tensor_array_name='EG_losses' )\n",
    "    w, chg_w, k, G_inf, losses = tf.while_loop( cond=_continue_cond, body=_eg_step, loop_vars=[w, chg_w, k, G_inf, losses] )     \n",
    "    return w, chg_w, k, G_inf, losses\n",
    "\n",
    "# test EG on a trivial problem\n",
    "#################################################################\n",
    "T = 100\n",
    "N = 100\n",
    "q = tf.Variable(name='weights', initial_value=np.ones(N),  dtype=DTYPE)\n",
    "c = tf.constant( np.arange(T), dtype=DTYPE)\n",
    "eta_0 = tf.Variable(name='eta0', initial_value=0.05,  dtype=DTYPE)\n",
    "\n",
    "def loss( q ):\n",
    "    '''return the loss function for a given set of parameters'''\n",
    "    return tf.reduce_sum( q * c )\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "qf, chg_w, k, G_inf, losses = eg(loss, q, eta_0=eta_0)\n",
    "print( loss(qf).eval(), '\\n', qf.eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solve the discrepancy problem\n",
    "####################################################\n",
    "%matplotlib nbagg\n",
    "from numba import jit\n",
    "from numpy.random import multinomial\n",
    "from pandas import *\n",
    "\n",
    "# generate the data from a markov process\n",
    "T = 10000\n",
    "N = 100\n",
    "R = 100\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def markov_sample( M, T=T, betas=betas, seed=1 ):\n",
    "    beta_seq = np.zeros( (T, N) )    \n",
    "    # initial state\n",
    "    r = multinomial( 1, np.ones(R)/R ).nonzero()[0][0]  \n",
    "    rs = np.empty(T)\n",
    "    # sample from the markov probability\n",
    "    for t in range(T):      \n",
    "        r = multinomial(1, pvals=M[r, :] ).nonzero()[0][0] \n",
    "        beta_seq[t,:] = betas[r, :]\n",
    "        rs[t] = r\n",
    "    return beta_seq, rs\n",
    "\n",
    "X = np.array( [ np.sin( np.arange(T) * 2 *  f / T * pi ) for f in arange(N) ]).T\n",
    "betas = randn( R, N )\n",
    "a = 0.01\n",
    "M = (1-a) *np.eye(R) * (R-1)/R + a * np.ones( (R,R) ) / R\n",
    "M = M / M.sum( axis=1)\n",
    "beta_seq, rs =  markov_sample(M )\n",
    "yorg = (X * beta_seq).sum(axis=1)\n",
    "y = yorg + 0.1 * randn(T)\n",
    "\n",
    "if False:\n",
    "    figure()\n",
    "    subplot(121)\n",
    "    imshow(beta_seq, aspect='auto')\n",
    "    subplot(122)\n",
    "    plot(rs, alpha=0.1)   \n",
    "\n",
    "X_trg, y_trg = X[0:T//2], y[0:T//2]\n",
    "X_tst, y_tst = X[T//2:], y[T//2:]\n",
    "\n",
    "# solve using OLS\n",
    "from statsmodels.api import OLS\n",
    "ols = OLS( exog=X_trg, endog=y_trg).fit()\n",
    "print('trg corr', Series( ols.predict( X_trg )).corr(Series(y_trg)) , 'tst corr', Series( ols.predict( X_tst)).corr(Series(y_tst) ) )\n",
    "\n",
    "\n",
    "# solve using discrpenacy\n",
    "# \\min_beta_T |sum_t q_t ||y_t - X_t \\beta_T||^2  + lambda ||beta_T||^2\n",
    "# where q_t = \\argmin_q | \\sum_t q_t L_t(\\beta_T) - \\sum_s L_s(\\beta_s) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "multinomial( 1, np.ones(R)/R).nonzero()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.squeeze(multinomial( 1, np.ones(R)/R, size=1)).nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "imshow(X, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Out[15][0].eval()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
